{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: request an API key\n",
    "\n",
    "https://regulationsgov.github.io/developers/key/\n",
    "\n",
    "Assign the key as a quoted string called `API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from my_secrets.keys import regulations as API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: download all NLTK data. This is necessary for the algorithms to work since they are based on *corpora* that are used in pattern matching.\n",
    "\n",
    "http://www.nltk.org/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a docket id and search for all comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_url = 'https://api.data.gov/regulations/v3/documents.json'\n",
    "\n",
    "payload = {\n",
    "    'api_key': API_KEY,\n",
    "    'dckid': 'EPA-HQ-OAR-2010-0505',\n",
    "    'dct': 'PS',\n",
    "    'rpp': 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['documents', 'totalNumRecords'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.get(document_url, params=payload)\n",
    "documents = response.json()\n",
    "documents.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This docket has over 6 million public submissions.\n",
    "\n",
    "With the potentially large volume of public comments it is best to filter first by using the `'s'` search query in the `payload`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6564595"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['totalNumRecords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## natural language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the 200 we received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments = documents['documents']\n",
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agencyAcronym': 'HHS',\n",
       " 'allowLateComment': False,\n",
       " 'attachmentCount': 0,\n",
       " 'commentDueDate': '2011-09-30T23:59:59-04:00',\n",
       " 'commentStartDate': '2011-08-03T00:00:00-04:00',\n",
       " 'commentText': 'Pregnancy is not a disease, and drugs and surgeries to prevent it are not basic health care that the government should require all Americans to purchase.  Please remove sterilization and prescription contraceptives from the list of \"preventive services\" the federal government is mandating in private health plans.  It is especially important to exclude any drug that may cause an early abortion, and to fully respect religious freedom as other federal laws do.  The narrow religious exemption in HHS\\'s new rule protects almost no one. I urge you to allow all organizations and individuals to offer, sponsor and obtain health coverage that does not violate their moral and religious convictions.',\n",
       " 'docketId': 'HHS-OS-2011-0023',\n",
       " 'docketTitle': 'Group Health Plans and Health Insurance Issuers Relating to Coverage of Preventive Services under the Patient Protection and Affordable Care Act',\n",
       " 'docketType': 'Rulemaking',\n",
       " 'documentId': 'HHS-OS-2011-0023-78792',\n",
       " 'documentStatus': 'Posted',\n",
       " 'documentType': 'Public Submission',\n",
       " 'numberOfCommentsReceived': 1,\n",
       " 'openForComment': False,\n",
       " 'postedDate': '2011-11-01T00:00:00-04:00',\n",
       " 'rin': 'Not Assigned',\n",
       " 'submitterName': 'AT Garcia',\n",
       " 'title': 'Comment on FR Doc # 2011-19684'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_comment = comments[3]\n",
    "a_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = a_comment['commentText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import some useful tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stoplist = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is considered best practice to first normalize the case of the text, unless performing parts of speech. \n",
    "\n",
    "Notice that punctuation are separated from the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pregnancy',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'disease',\n",
       " ',',\n",
       " 'and',\n",
       " 'drugs',\n",
       " 'and',\n",
       " 'surgeries',\n",
       " 'to',\n",
       " 'prevent',\n",
       " 'it',\n",
       " 'are',\n",
       " 'not',\n",
       " 'basic',\n",
       " 'health',\n",
       " 'care',\n",
       " 'that',\n",
       " 'the',\n",
       " 'government',\n",
       " 'should',\n",
       " 'require',\n",
       " 'all',\n",
       " 'americans',\n",
       " 'to',\n",
       " 'purchase',\n",
       " '.',\n",
       " 'please',\n",
       " 'remove']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(text.lower())\n",
    "words[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter out to important words we need to\n",
    "1. remove short words, called `stopwords`\n",
    "2. remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_words = []\n",
    "for word in words:\n",
    "    \n",
    "    # skip anything that does not contain letters\n",
    "    if not word.isalpha():\n",
    "        continue\n",
    "    \n",
    "    if word not in stoplist:\n",
    "        important_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pregnancy',\n",
       " 'disease',\n",
       " 'drugs',\n",
       " 'surgeries',\n",
       " 'prevent',\n",
       " 'basic',\n",
       " 'health',\n",
       " 'care',\n",
       " 'government',\n",
       " 'require']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over all of the comments and find the important words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def important(comment):\n",
    "    words = word_tokenize(comment.lower())\n",
    "    \n",
    "    important_words = []\n",
    "    for word in words:\n",
    "    \n",
    "        # skip anything that does not contain letters\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "    \n",
    "        if word not in stoplist:\n",
    "            important_words.append(word)\n",
    "    \n",
    "    return important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_important_words = []\n",
    "\n",
    "for comment in comments:\n",
    "    \n",
    "    # some items have not text?\n",
    "    if 'commentText' in comment:\n",
    "        text = comment['commentText']\n",
    "    \n",
    "    all_important_words.extend(important(text))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13568"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_important_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a flat list of all of the important words let's perform some statistical analysis.\n",
    "\n",
    "The `FreqDist` method let's create frequency distributions to determine the most common words. The stop words and punctuation have already been filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distributions = FreqDist(all_important_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 10 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('national', 138),\n",
       " ('public', 95),\n",
       " ('would', 89),\n",
       " ('health', 75),\n",
       " ('monuments', 73),\n",
       " ('please', 67),\n",
       " ('one', 64),\n",
       " ('act', 61),\n",
       " ('proposed', 57),\n",
       " ('use', 55)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributions.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the 10 most infrequently used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mainland', 1),\n",
       " ('hawaii', 1),\n",
       " ('island', 1),\n",
       " ('listen', 1),\n",
       " ('deaf', 1),\n",
       " ('interim', 1),\n",
       " ('whats', 1),\n",
       " ('fructose', 1),\n",
       " ('syrup', 1),\n",
       " ('catch', 1)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distributions.most_common()[-10:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-training]",
   "language": "python",
   "name": "conda-env-anaconda-training-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
